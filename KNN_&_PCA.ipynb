{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "ans-          \n",
        "\n",
        "    The k-nearest neighbors (KNN) algorithm is a non-parametric, supervised\n",
        "    \n",
        "    learning classifier, which uses proximity to make classifications or\n",
        "    \n",
        "    predictions about the grouping of an individual data point."
      ],
      "metadata": {
        "id": "iBxpkj9NYKkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "]\n",
        "ans-\n",
        "\n",
        "    The Curse of Dimensionality refers to various phenomena that arise when\n",
        "    \n",
        "    dealing with high-dimensional data.\n",
        "\n",
        "**How it affects KNN performance:**\n",
        "\n",
        "\n",
        "1- **Misleading Distances:**\n",
        "\n",
        "    In high dimensions, all points seem far apart, so the \"nearest\" neighbors\n",
        "    \n",
        "    might not be truly close or representative, as noted in Towards AI.\n",
        "\n",
        "\n",
        "\n",
        "2- **Increased Data Needs:**\n",
        "\n",
        "    To maintain density and find reliable neighbors, KNN needs an exponentially\n",
        "       \n",
        "    larger dataset as dimensions increase, quickly becoming impractical.\n",
        "\n",
        "\n",
        "3- **Overfitting:**\n",
        "\n",
        "\n",
        "    With sparse data, KNN can latch onto irrelevant features (noise), leading\n",
        "    \n",
        "    to poor generalization on new data, as it relies heavily on local patterns.\n",
        "\n",
        "\n",
        "4- **Computational Cost**:\n",
        "\n",
        "    Calculating distances in high-dimensional spaces is computationally\n",
        "    \n",
        "    expensive and slow, increasing training/prediction time.\n",
        "\n",
        "\n",
        "5- **Loss of Discriminative Power:**\n",
        "\n",
        "\n",
        "    The core principle of KNN—finding similar neighbors—breaks down because all\n",
        "    \n",
        "    points become effectively \"dissimilar\" and distant, making classifications unreliable"
      ],
      "metadata": {
        "id": "9evS0FuTYpl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Principal Component Analysis (PCA) is a feature extraction method that\n",
        "    \n",
        "    creates new, fewer, uncorrelated \"artificial\" features (components) that\n",
        "    \n",
        "    capture most data variance, ideal for dimensionality reduction, while\n",
        "    \n",
        "    Feature Selection directly picks the most relevant original features,\n",
        "    \n",
        "    keeping them intact, often using target labels to assess importance,\n",
        "    \n",
        "    making PCA transformative but less interpretable, whereas feature\n",
        "     \n",
        "    selection keeps data's original meaning but might miss complex patterns"
      ],
      "metadata": {
        "id": "uyzX32vQaoMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "ans-\n",
        "    \n",
        "    In PCA, eigenvectors define the new axes (Principal Components) of the\n",
        "    \n",
        "    data, representing directions of maximum variance, while eigenvalues\n",
        "    \n",
        "    quantify the amount of variance along each eigenvector, indicating its\n",
        "    \n",
        "    importance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " **Why They Are Important in PCA**\n",
        "\n",
        "\n",
        "**Dimensionality Reduction:**\n",
        "\n",
        "    By sorting eigenvectors by their eigenvalues (largest first), we identify\n",
        "    \n",
        "    the most informative directions, allowing us to discard components with\n",
        "    \n",
        "    small eigenvalues, thus reducing dimensions.\n",
        "\n",
        "\n",
        "**Information Preservation**:\n",
        "\n",
        "    Focusing on components with high eigenvalues ensures we keep the most\n",
        "    \n",
        "    significant patterns and variability in the data.\n",
        "\n",
        "\n",
        "**Data Transformation**:\n",
        "\n",
        "    They transform data from its original feature space to a new, smaller\n",
        "    \n",
        "    \n",
        "    principal component space, making data easier to visualize and process.\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "JJM7f4OmbF2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "ans -\n",
        "\n",
        "\n",
        "**KNN and PCA can be combined in a single pipeline to enhance the performance and efficiency of machine learning models [1]. The techniques complement each other in two primary ways**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "1- **Noise Reduction:**\n",
        "\n",
        "\n",
        "    PCA identifies and isolates the most significant variance in the data,\n",
        "\n",
        "    filtering out minor, random variations (noise) present in higher dimensions.\n",
        "\n",
        "    KNN is sensitive to noise, so applying PCA first can lead to more robust and\n",
        "\n",
        "    accurate classification or regression results by focusing on the underlying\n",
        "\n",
        "    structure of the data\n",
        "\n",
        "\n",
        "\n",
        "2- **Dimensionality Reduction for Efficiency**:\n",
        "\n",
        "\n",
        "    PCA reduces the number of dimensions in the dataset. This is highly\n",
        "    \n",
        "    beneficial for KNN because the computational cost and memory requirements\n",
        "    \n",
        "    of KNN increase significantly with the number of dimensions (a phenomenon\n",
        "     \n",
        "    known as the \"curse of dimensionality\") . By reducing the dimensionality,\n",
        "      \n",
        "    PCA makes the KNN algorithm run much faster and with less memory\n",
        "       \n",
        "    overhead."
      ],
      "metadata": {
        "id": "mvYqIa5Ub4nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "\n"
      ],
      "metadata": {
        "id": "eNZKPTC9coWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Case 1: KNN without Feature Scaling ---\n",
        "\n",
        "# 3. Initialize and train the KNN classifier (without scaling)\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions and evaluate\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --- Case 2: KNN with Feature Scaling ---\n",
        "\n",
        "# 5. Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 6. Fit the scaler on the training data and transform both training and test data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 7. Initialize and train the KNN classifier (with scaling)\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 8. Make predictions and evaluate\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# 9. Compare accuracies\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMOushqPcvGc",
        "outputId": "9e51efd0-92a9-49d4-cd72-d1163aba830b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407\n",
            "Accuracy with scaling:    0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component."
      ],
      "metadata": {
        "id": "UV9KzXlUdL-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd # For better display of results\n",
        "\n",
        "# 2. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "# y = wine.target # Target (wine type) for potential classification, but not needed for basic PCA variance\n",
        "\n",
        "# 3. Standardize the data (Crucial for PCA!)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 4. Initialize and Train PCA\n",
        "# We'll let PCA decide components, but can specify n_components=None to see all\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# 5. Print Explained Variance Ratio\n",
        "print(\"Explained Variance Ratio for each Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n",
        "\n",
        "# Optional: Print cumulative variance\n",
        "print(\"\\nCumulative Explained Variance:\")\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "for i, cum_ratio in enumerate(cumulative_variance):\n",
        "    print(f\"Up to PC{i+1}: {cum_ratio:.4f}\")\n",
        "\n",
        "# Optional: Display results in a DataFrame for clarity\n",
        "results_df = pd.DataFrame({\n",
        "    'Principal Component': [f'PC{i+1}' for i in range(len(pca.explained_variance_ratio_))],\n",
        "    'Explained Variance Ratio': pca.explained_variance_ratio_,\n",
        "    'Cumulative Variance': cumulative_variance\n",
        "})\n",
        "print(\"\\n--- PCA Results ---\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNa9OeSmdRXB",
        "outputId": "35a26b2f-5b0a-4b83-fd08-c4fed3c3a113"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio for each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n",
            "\n",
            "Cumulative Explained Variance:\n",
            "Up to PC1: 0.3620\n",
            "Up to PC2: 0.5541\n",
            "Up to PC3: 0.6653\n",
            "Up to PC4: 0.7360\n",
            "Up to PC5: 0.8016\n",
            "Up to PC6: 0.8510\n",
            "Up to PC7: 0.8934\n",
            "Up to PC8: 0.9202\n",
            "Up to PC9: 0.9424\n",
            "Up to PC10: 0.9617\n",
            "Up to PC11: 0.9791\n",
            "Up to PC12: 0.9920\n",
            "Up to PC13: 1.0000\n",
            "\n",
            "--- PCA Results ---\n",
            "   Principal Component  Explained Variance Ratio  Cumulative Variance\n",
            "0                  PC1                  0.361988             0.361988\n",
            "1                  PC2                  0.192075             0.554063\n",
            "2                  PC3                  0.111236             0.665300\n",
            "3                  PC4                  0.070690             0.735990\n",
            "4                  PC5                  0.065633             0.801623\n",
            "5                  PC6                  0.049358             0.850981\n",
            "6                  PC7                  0.042387             0.893368\n",
            "7                  PC8                  0.026807             0.920175\n",
            "8                  PC9                  0.022222             0.942397\n",
            "9                 PC10                  0.019300             0.961697\n",
            "10                PC11                  0.017368             0.979066\n",
            "11                PC12                  0.012982             0.992048\n",
            "12                PC13                  0.007952             1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n"
      ],
      "metadata": {
        "id": "vG5C9a0UdhSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris # Example dataset\n",
        "\n",
        "# 1. Load and Preprocess the Dataset\n",
        "# We'll use the Iris dataset as an example.\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) #\n",
        "\n",
        "# Standardize the data (crucial for both PCA and KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 2. Train KNN on the Original (Scaled) Dataset\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5) #\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# 3. Apply PCA and Train KNN on Transformed Dataset\n",
        "# Retain top 2 components\n",
        "pca = PCA(n_components=2) #\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled) # Apply the same transformation to test set\n",
        "\n",
        "# Train KNN on the PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# 4. Compare Accuracies\n",
        "print(f\"Accuracy on original dataset: {accuracy_original:.4f}\")\n",
        "print(f\"Accuracy on PCA-transformed dataset (2 components): {accuracy_pca:.4f}\")\n",
        "\n",
        "# You can also print the explained variance ratio for context\n",
        "print(f\"Explained variance of the top 2 components: {pca.explained_variance_ratio_.sum():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGdcdY-ydmd3",
        "outputId": "68a0c9e3-b030-4e28-e45f-18664c037d57"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset: 1.0000\n",
            "Accuracy on PCA-transformed dataset (2 components): 0.9556\n",
            "Explained variance of the top 2 components: 0.9521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "8Vp5Ldx2dtrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "# Using a random state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# 3. Scale the features\n",
        "# Feature scaling is crucial for distance-based algorithms like KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Function to train and evaluate KNN with a specific metric\n",
        "def train_and_evaluate_knn(metric_name):\n",
        "    # 'euclidean' is the default metric (p=2), 'manhattan' is p=1\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric_name)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# 4. Train and compare models\n",
        "euclidean_accuracy = train_and_evaluate_knn('euclidean')\n",
        "manhattan_accuracy = train_and_evaluate_knn('manhattan')\n",
        "\n",
        "# 5. Display results\n",
        "print(f\"Accuracy with Euclidean distance: {euclidean_accuracy:.4f}\")\n",
        "print(f\"Accuracy with Manhattan distance: {manhattan_accuracy:.4f}\")\n",
        "\n",
        "# Compare and summarise\n",
        "if euclidean_accuracy > manhattan_accuracy:\n",
        "    print(\"\\nEuclidean distance performed better.\")\n",
        "elif manhattan_accuracy > euclidean_accuracy:\n",
        "    print(\"\\nManhattan distance performed better.\")\n",
        "else:\n",
        "    print(\"\\nBoth distance metrics performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3UGF-YhdymS",
        "outputId": "42c92ef8-4c22-407a-8e84-8c91afc90fb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9444\n",
            "Accuracy with Manhattan distance: 0.9815\n",
            "\n",
            "Manhattan distance performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n"
      ],
      "metadata": {
        "id": "VhAlCF-Wd9NO"
      }
    }
  ]
}