{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        " ans-  \n",
        "    Information Gain measures how much a feature reduces uncertainty (entropy)\n",
        "\n",
        "    in a dataset. In decision trees, it is used to select the best feature to\n",
        "     \n",
        "    split the data at each node by choosing the feature with the highest\n",
        "     \n",
        "    information gain, which leads to the most pure or homogeneous child nodes.\n",
        "     \n",
        "    This process is repeated recursively to build the tree, effectively\n",
        "      \n",
        "    creating a series of if-then rules for prediction.\n"
      ],
      "metadata": {
        "id": "b0EmqfGTirEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "ans-  \n",
        "\n",
        "\n",
        "    Gini Impurity and Entropy are both measures of a node's impurity in a decision\n",
        "\n",
        "    tree, but they differ in their calculation and range. Gini Impurity is faster\n",
        "\n",
        "    to compute and ranges from 0 to 0.5, while Entropy uses logarithms, is\n",
        "\n",
        "    more computationally expensive, and its values range from 0 to 1. Both\n",
        "\n",
        "    aim to find the best split by minimizing impurity, but Gini is often\n",
        "  \n",
        "    preferred for large datasets due to its speed, as the results from both are\n",
        "  \n",
        "    usually very similar."
      ],
      "metadata": {
        "id": "tJFVlM6vd3M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "    it is a technique in decision tree that halts the growth of tree during its\n",
        "    \n",
        "    construction to prevent it from becoming too complex and over fitting the\n",
        "     \n",
        "    training data ."
      ],
      "metadata": {
        "id": "_04j8XkTegzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances"
      ],
      "metadata": {
        "id": "6eDqPfE8fVYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris # Example dataset\n",
        "\n",
        "# Load an example dataset (Iris dataset)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini impurity as the criterion\n",
        "# criterion='gini' is the default, but explicitly setting it emphasizes the choice.\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(X.columns, dt_classifier.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xW82fHpfX0y",
        "outputId": "6e0ad86f-524e-4229-cf28-8bc495e318a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        " ans-  \n",
        "\n",
        "    A Support Vector Machine (SVM) is a supervised machine learning algorithm\n",
        "\n",
        "    used for classification and regression tasks by finding an optimal hyperplane\n",
        "  \n",
        "    to separate data points into different classes"
      ],
      "metadata": {
        "id": "NgU416wxfvF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "\n",
        "    The Kernel Trick is a method used in Support Vector Machines (SVM) to\n",
        "    \n",
        "    classify non-linear data without explicitly converting it to a\n",
        "    \n",
        "    higher-dimensional space. It works by using a kernel function to compute\n",
        "    \n",
        "    the dot product between data points in that higher-dimensional space, which\n",
        "    \n",
        "    is computationally more efficient than actually performing the transformation"
      ],
      "metadata": {
        "id": "G9BlnKi0gDTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "qyMLFvk2gRBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with a Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the Linear SVM\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "\n",
        "# Train an SVM classifier with an RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the RBF SVM\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"Linear Kernel performed better.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"RBF Kernel performed better.\")\n",
        "else:\n",
        "    print(\"Both kernels performed equally well.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeS0Ks5igVA-",
        "outputId": "38408826-bd1b-4d50-f98c-b279059cbb4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.9815\n",
            "Accuracy of SVM with RBF Kernel: 0.7593\n",
            "Linear Kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 8: What is the Naive Bayes classifier, and why is it called \"Naive\"?\n",
        "\n",
        " ans-\n",
        "\n",
        "\n",
        "    The Naive Bayes classifier is a simple, probabilistic classification\n",
        "    \n",
        "    algorithm that uses Bayes' Theorem to predict the probability of a class\n",
        "    \n",
        "    for a given data point. It is called \"naïve\" because it makes a strong,\n",
        "    \n",
        "    often unrealistic, assumption that all features in the data are independent\n",
        "    \n",
        "    of each other."
      ],
      "metadata": {
        "id": "7OG6Ol64ggcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 9: Explain the differences between Gaussian Naive Bayes, Multinomial Naive\n",
        "Bayes, and Bernoulli Naive Bayes\n",
        "\n",
        "ans-\n",
        "\n",
        "**Gaussian naive**\n",
        "\n",
        "    handles continuous data by assuming it follows a bell curve (normal distribution).\n",
        "\n",
        "\n",
        "**Multinomial naive**\n",
        "\n",
        "    works with discrete data, specifically counts or frequencies (how many\n",
        "    \n",
        "    times something appears).\n",
        "\n",
        "\n",
        "**Bernoulli naive**  \n",
        "\n",
        "    is for binary data, where a feature is either present (1) or absent (0)."
      ],
      "metadata": {
        "id": "KgRPHC7WgyPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "ldNPuIlZhZbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "# The dataset is included in scikit-learn for convenience\n",
        "print(\"Loading the Breast Cancer dataset...\")\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "print(f\"Dataset loaded. X shape: {X.shape}, y shape: {y.shape}\\n\")\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "# We use 70% for training and 30% for testing\n",
        "# A random state is set for reproducibility\n",
        "print(\"Splitting data into training (70%) and testing (30%) sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\\n\")\n",
        "\n",
        "# 3. Initialize the Gaussian Naive Bayes classifier\n",
        "# GaussianNB is suitable for continuous data, like the features in this dataset\n",
        "print(\"Initializing Gaussian Naive Bayes classifier...\")\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# 4. Train the classifier on the training data\n",
        "print(\"Training the classifier...\")\n",
        "gnb.fit(X_train, y_train)\n",
        "print(\"Training complete.\\n\")\n",
        "\n",
        "# 5. Make predictions on the testing data\n",
        "print(\"Making predictions on the test set...\")\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(\"Predictions complete.\\n\")\n",
        "\n",
        "# 6. Evaluate the accuracy of the model\n",
        "# Compare the predicted labels (y_pred) with the actual labels (y_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Optional: Display a few actual vs. predicted results\n",
        "print(\"\\nSample Actual vs. Predicted Labels:\")\n",
        "for i in range(10):\n",
        "    print(f\"  Actual: {y_test[i]}, Predicted: {y_pred[i]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InVcNH7vicM5",
        "outputId": "0eb643fb-c486-4ea4-ffa3-0ac67dc61631"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the Breast Cancer dataset...\n",
            "Dataset loaded. X shape: (569, 30), y shape: (569,)\n",
            "\n",
            "Splitting data into training (70%) and testing (30%) sets...\n",
            "Training set size: 398 samples\n",
            "Testing set size: 171 samples\n",
            "\n",
            "Initializing Gaussian Naive Bayes classifier...\n",
            "Training the classifier...\n",
            "Training complete.\n",
            "\n",
            "Making predictions on the test set...\n",
            "Predictions complete.\n",
            "\n",
            "----------------------------------------\n",
            "Model Accuracy on Test Set: 94.15%\n",
            "----------------------------------------\n",
            "\n",
            "Sample Actual vs. Predicted Labels:\n",
            "  Actual: 1, Predicted: 1\n",
            "  Actual: 0, Predicted: 0\n",
            "  Actual: 0, Predicted: 0\n",
            "  Actual: 1, Predicted: 1\n",
            "  Actual: 1, Predicted: 1\n",
            "  Actual: 0, Predicted: 0\n",
            "  Actual: 0, Predicted: 0\n",
            "  Actual: 0, Predicted: 0\n",
            "  Actual: 1, Predicted: 1\n",
            "  Actual: 1, Predicted: 1\n"
          ]
        }
      ]
    }
  ]
}